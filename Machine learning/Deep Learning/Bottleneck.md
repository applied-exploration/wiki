If you train an encoder and a decoder with a bottleneck in between, you force the network to learn a useful, more general and compressed representation of the problem (often called the code or latent space). An example of [[Feature learning]]
The quintessential example of this is the [[Autoencoder]], it's task is to recreate it's input to a high fidelity, while only capturing the most meaningful aspects of it.

[[Compression]]
[[Dimensionality reduction]]
[[Attention]]
